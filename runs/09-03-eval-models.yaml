description: evaluating models

target:
  service: sing
  # which virtual cluster you belong to (msrlabs, etc.). Everyone has access to "msrlabs".
  # physical cluster to use (cam, gcr, rr1, rr2) or Azure clusters (eu1, eu2, etc.)
  name:  msroctovc # msrresrchvc #
  

environment:
  # registry: singularitybase.azurecr.io
  image: amlt-sing/acpt-2.4.0-py3.10-cuda12.4:20240731T103952913 # base/job/pytorch/acpt-2.2.1-py3.10-cuda12.1:20240312T225111416
  setup:
    - python -m pip install -r requirements.txt --user

code:
  local_dir: $CONFIG_DIR/..


search:
  job_template:
    name: grid_{experiment_name:s}_{auto:3s}
    sku: G1-A100
    identity: managed
    sla_tier: premium
    command:
    - python src/lm_eval.py model.name={model_name} lm_eval.model_path={model_name} lm_eval.is_unwatermarked_model=True lm_eval.gpu_memory_utilization=0.5

  type: hyperdrive
  sampling: grid
  max_trials: 1000
  parallel_trials: 1000
  params:
    
    - name: model_name
      spec: discrete
      values: ['meta-llama/Meta-Llama-3.1-8B'] # ['meta-llama/Llama-2-7b-hf', 'mistralai/Mistral-7B-v0.3', 'microsoft/Phi-3-mini-4k-instruct', 'meta-llama/Meta-Llama-3.1-8B'] # ['google/gemma-2-2b']


